from .base import LLMProvider
import os
from openai import OpenAI
from typing import Generator, Tuple

class GPT4oProvider(LLMProvider):
    """
    An LLM provider that uses the OpenAI API to serve the GPT-4o model.
    """
    def __init__(self):
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        self.model = "gpt-4o"

    def query(self, system_prompt: str, user_prompt: str) -> Generator[str, None, None]:
        """
        Sends a streaming query to the GPT-4o model via the OpenAI API.

        Args:
            system_prompt: The instruction or context for the model's behavior.
            user_prompt: The user's direct question or input.

        Yields:
            Chunks of the text response as they are generated by the LLM.
        """
        # First, make a non-streaming call to get token usage information
        completion = self.client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": user_prompt,
                }
            ],
            model=self.model,
            stream=False,
        )

        # Extract and log token usage
        if completion.usage:
            input_tokens = completion.usage.prompt_tokens
            output_tokens = completion.usage.completion_tokens
            total_tokens = completion.usage.total_tokens
            print(f"ðŸ”¢ GPT-4o Token Usage - Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}")
        
        # Return the response content as a generator (simulating streaming)
        response_content = completion.choices[0].message.content
        if response_content:
            yield response_content 